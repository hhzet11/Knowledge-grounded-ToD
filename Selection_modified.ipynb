{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31468,"status":"ok","timestamp":1683521616385,"user":{"displayName":"이하영","userId":"16656722516517244964"},"user_tz":-540},"id":"NT6ZWQjgQBh9","outputId":"8a5ebeab-8bcf-455b-c2aa-890668e6a8a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Erfk7KTFQFKn","executionInfo":{"status":"ok","timestamp":1683521616386,"user_tz":-540,"elapsed":15,"user":{"displayName":"이하영","userId":"16656722516517244964"}}},"outputs":[],"source":["!cd \"/content/drive/MyDrive/Knowledge-grounded Task-oriented Dialogue system\""]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109179,"status":"ok","timestamp":1683521725554,"user":{"displayName":"이하영","userId":"16656722516517244964"},"user_tz":-540},"id":"SsfhqdJFQkbn","outputId":"6a51554e-9202-48d1-d372-611e14d93197"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.13.1\n","  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1) (4.5.0)\n","Collecting nvidia-cudnn-cu11==8.5.0.96\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.40.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (67.7.2)\n","Installing collected packages: nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.0.0+cu118\n","    Uninstalling torch-2.0.0+cu118:\n","      Successfully uninstalled torch-2.0.0+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n","torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tqdm==4.62.3\n","  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tqdm\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.65.0\n","    Uninstalling tqdm-4.65.0:\n","      Successfully uninstalled tqdm-4.65.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tqdm-4.62.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"]}],"source":["!pip install torch==1.13.1\n","!pip install tqdm==4.62.3\n","!pip install transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"5snqtvlVQUjc","executionInfo":{"status":"ok","timestamp":1683521727658,"user_tz":-540,"elapsed":2117,"user":{"displayName":"이하영","userId":"16656722516517244964"}}},"outputs":[],"source":["import os\n","import json\n","import torch\n","import random\n","import copy\n","\n","import pandas as pd\n","\n","from tqdm import tqdm\n","from collections import defaultdict"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"JfaQX4SOQPkD","executionInfo":{"status":"ok","timestamp":1683521727659,"user_tz":-540,"elapsed":14,"user":{"displayName":"이하영","userId":"16656722516517244964"}}},"outputs":[],"source":["#scripts/dataset_walker\n","class DatasetWalker(object):\n","    def __init__(self, dataset, dataroot, labels=False, labels_file=None, incl_knowledge=False):\n","        path = os.path.join(os.path.abspath(dataroot))\n","            \n","        if dataset not in ['train', 'val']:\n","            raise ValueError('Wrong dataset name: %s' % (dataset))\n","\n","        logs_file = os.path.join(path, dataset, 'logs.json')\n","        with open(logs_file, 'r') as f:\n","            self.logs = json.load(f)\n","\n","        self.labels = None\n","\n","        if labels is True:\n","            if labels_file is None:\n","                labels_file = os.path.join(path, dataset, 'labels.json')\n","\n","            with open(labels_file, 'r') as f:\n","                self.labels = json.load(f)\n","\n","        self._incl_knowledge = incl_knowledge\n","        if self._incl_knowledge is True:\n","            # knowledge_reader 수정\n","            #self._knowledge = knowledge_reader(dataroot)\n","            self._knowledge = KnowledgeReader(dataroot)\n","\n","    def __iter__(self):\n","        if self.labels is not None:\n","            for log, label in zip(self.logs, self.labels):\n","                if self._incl_knowledge is True and label['target'] is True:\n","                    for idx, snippet in enumerate(label['knowledge']):\n","                        domain = snippet['domain']\n","                        entity_id = snippet['entity_id']\n","                        doc_type = snippet['doc_type']\n","                        doc_id = snippet['doc_id']\n","\n","                        if doc_type == 'review':\n","                            sent_id = snippet['sent_id']                            \n","                            sent = self._knowledge.get_review_sent(domain, entity_id, doc_id, sent_id)\n","                            label['knowledge'][idx]['sent'] = sent\n","                            \n","                        elif doc_type == 'faq':\n","                            doc = self._knowledge.get_faq_doc(domain, entity_id, doc_id)\n","                            question = doc['question']\n","                            answer = doc['answer']\n","\n","                            label['knowledge'][idx]['question'] = question\n","                            label['knowledge'][idx]['answer'] = answer\n","                \n","                yield(log, label)\n","        else:\n","            for log in self.logs:\n","                yield(log, None)\n","\n","    def __len__(self, ):\n","        return len(self.logs)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KyKvyQdJQYDa","executionInfo":{"status":"ok","timestamp":1683521727659,"user_tz":-540,"elapsed":11,"user":{"displayName":"이하영","userId":"16656722516517244964"}}},"outputs":[],"source":["#scripts/knowledge_reader\n","class KnowledgeReader(object):\n","    def __init__(self, dataroot, knowledge_file='knowledge.json'):\n","        path = os.path.join(os.path.abspath(dataroot))\n","\n","        with open(os.path.join(path, knowledge_file), 'r') as f:\n","            self.knowledge = json.load(f)\n","\n","    def get_domain_list(self):\n","        return list(self.knowledge.keys())\n","\n","    def get_entity_list(self, domain):\n","        if domain not in self.get_domain_list():\n","            raise ValueError(\"invalid domain name\")\n","\n","        entity_ids = []\n","        for entity_id in self.knowledge[domain].keys():\n","            entity_ids.append(int(entity_id))\n","\n","        result = []\n","        for entity_id in sorted(entity_ids):\n","            entity_name = self.knowledge[domain][str(entity_id)]['name']\n","            result.append({'id': entity_id, 'name': entity_name})\n","\n","        return result\n","\n","    def get_entity_name(self, domain, entity_id):\n","        if domain not in self.get_domain_list():\n","            raise ValueError(\"invalid domain name: %s\" % domain)\n","\n","        if str(entity_id) not in self.knowledge[domain]:\n","            raise ValueError(\"invalid entity id: %s\" % str(entity_id))\n","\n","        result = self.knowledge[domain][str(entity_id)]['name'] or None\n","\n","        return result\n","\n","    def get_faq_doc_ids(self, domain, entity_id):\n","        if domain not in self.get_domain_list():\n","            raise ValueError(\"invalid domain name: %s\" % domain)\n","        \n","        result = []\n","\n","        if str(entity_id) not in self.knowledge[domain]:\n","            raise ValueError(\"invalid entity id: %s\" % str(entity_id))\n","\n","        entity_obj = self.knowledge[domain][str(entity_id)]\n","        for doc_id, doc_obj in entity_obj['faqs'].items():\n","            result.append(doc_id)\n","\n","        return result\n","\n","    def get_faq_doc(self, domain, entity_id, doc_id):\n","        if domain not in self.get_domain_list():\n","            raise ValueError(\"invalid domain name: %s\" % domain)\n","\n","        if str(entity_id) not in self.knowledge[domain]:\n","            raise ValueError(\"invalid entity id: %s\" % str(entity_id))\n","\n","        entity_name = self.get_entity_name(domain, entity_id)\n","\n","        if str(doc_id) not in self.knowledge[domain][str(entity_id)]['faqs']:\n","            raise ValueError(\"invalid doc id: %s\" % str(doc_id))\n","\n","        doc_obj = self.knowledge[domain][str(entity_id)]['faqs'][str(doc_id)]\n","        result = {'domain': domain, 'entity_id': entity_id, 'entity_name': entity_name, 'doc_id': doc_id, 'question': doc_obj['question'], 'answer': doc_obj['answer']}\n","\n","        return result\n","\n","    def get_review_doc_ids(self, domain, entity_id):\n","        if domain not in self.get_domain_list():\n","            raise ValueError(\"invalid domain name: %s\" % domain)\n","\n","        if str(entity_id) not in self.knowledge[domain]:\n","            raise ValueError(\"invalid entity id: %s\" % str(entity_id))\n","\n","        result = []\n","        \n","        entity_obj = self.knowledge[domain][str(entity_id)]\n","        for doc_id, doc_obj in entity_obj['reviews'].items():\n","            result.append(doc_id)\n","\n","        return result\n","\n","    def get_review_doc(self, domain, entity_id, doc_id):\n","        if domain not in self.get_domain_list():\n","            raise ValueError(\"invalid domain name: %s\" % domain)\n","\n","        if str(entity_id) not in self.knowledge[domain]:\n","            raise ValueError(\"invalid entity id: %s\" % str(entity_id))\n","\n","        entity_name = self.get_entity_name(domain, entity_id)\n","\n","        if str(doc_id) not in self.knowledge[domain][str(entity_id)]['reviews']:\n","            raise ValueError(\"invalid doc id: %s\" % str(doc_id))\n","        \n","        doc_obj = self.knowledge[domain][str(entity_id)]['reviews'][str(doc_id)]\n","        \n","        result = {'domain': domain, 'entity_id': entity_id, 'entity_name': entity_name, 'doc_id': doc_id, 'sentences': doc_obj['sentences']}\n","        if 'traveler_type' in doc_obj:\n","            result['traveler_type'] = doc_obj['traveler_type']\n","        \n","        if 'dishes' in doc_obj:\n","            result['dishes'] = doc_obj['dishes']\n","\n","        if 'drinks' in doc_obj:\n","            result['drinks'] = doc_obj['drinks']\n","\n","        return result\n","    \n","    def get_review_sent(self, domain, entity_id, doc_id, sent_id):\n","        if domain not in self.get_domain_list():\n","            raise ValueError(\"invalid domain name: %s\" % domain)\n","\n","        if str(entity_id) not in self.knowledge[domain]:\n","            raise ValueError(\"invalid entity id: %s\" % str(entity_id))\n","        \n","        if str(doc_id) not in self.knowledge[domain][str(entity_id)]['reviews']:\n","            raise ValueError(\"invalid doc id: %s\" % str(doc_id))\n","\n","        if str(sent_id) not in self.knowledge[domain][str(entity_id)]['reviews'][str(doc_id)]['sentences']:\n","            raise ValueError(\"invalid sentence id: %s\" % str(sent_id))\n","\n","        result = self.knowledge[domain][str(entity_id)]['reviews'][str(doc_id)]['sentences'][str(sent_id)]\n","\n","        return result"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"LLgf7egEQbgR","executionInfo":{"status":"ok","timestamp":1683521727660,"user_tz":-540,"elapsed":11,"user":{"displayName":"이하영","userId":"16656722516517244964"}}},"outputs":[],"source":["task = \"selection\"\n","dataroot = '/content/drive/MyDrive/Knowledge-grounded Task-oriented Dialogue system/Knowledge-grounded-ToD/data'\n","negative_sample_method = 'all'\n","knowledge_file = 'knowledge.json'\n","eval_only = False\n","\n","class BaseDataset(torch.utils.data.Dataset):\n","    global dstc11\n","    global dstc11_val \n","\n","    df = pd.DataFrame({\"history\" : 'aaa', \"knowledge_keys\": 'bbb', \"knowledge\" : 'ccc', \"candidates_keys\": 'ddd', 'candidates' : 'eee'}, index = [0])\n","    dstc11 = pd.DataFrame(df, columns = [\"history\", \"knowledge_keys\", \"knowledge\", \"candidates_keys\", \"candidates\"])\n","    dstc11_val = pd.DataFrame(df, columns = [\"history\", \"knowledge_keys\", \"knowledge\", \"candidates_keys\", \"candidates\"])\n","\n","    #df = pd.DataFrame({\"knowledge\" : 'aaa'}, index = [0])\n","    #knowledge = pd.DataFrame(df, columns = ['knowledge'])\n","\n","    def __init__(self, split_type, labels=True, labels_file=None):\n","        self.dataroot = dataroot\n","        self.split_type = split_type\n","        self.task = task\n","        self.negative_sample_method = negative_sample_method\n","\n","        self.dataset_walker = DatasetWalker(split_type, labels=labels, dataroot=self.dataroot, labels_file=labels_file)\n","        self.dialogs = self._prepare_conversations()\n","        self.knowledge_reader = KnowledgeReader(self.dataroot, knowledge_file)\n","        self.snippets = self._prepare_knowledge()\n","        self._create_examples()\n","\n","    def _prepare_conversations(self):\n","        \"\"\" Tokenize and encode the dialog data \"\"\"\n","        dialogs = []\n","        for i, (log, label) in enumerate(tqdm(self.dataset_walker, disable=False)):\n","            dialog = {}\n","            dialog[\"id\"] = i\n","            dialog[\"log\"] = log\n","            dialog[\"label\"] = label\n","            dialogs.append(dialog)\n","        return dialogs\n","\n","    def _prepare_knowledge(self):\n","        \"\"\" Tokenize and encode the knowledge snippets \"\"\"\n","        self.knowledge_docs = self._get_snippet_list()\n","\n","        snippets = defaultdict(dict)\n","        for snippet_id, snippet in enumerate(self.knowledge_docs):\n","            key = \"{}__{}__{}\".format(snippet[\"domain\"], str(snippet[\"entity_id\"]) or \"\", snippet[\"doc_id\"])\n","            knowledge = self._knowledge_to_string(snippet[\"doc\"], name=snippet[\"entity_name\"] or \"\")\n","            snippets[key] = knowledge\n","\n","        return snippets\n","\n","    def _get_snippet_list(self):\n","        \"\"\" Get all knowledge snippets in the dataset \"\"\"\n","        result = []\n","        i = 0\n","        for domain in self.knowledge_reader.get_domain_list():\n","            for entity_id in self.knowledge_reader.knowledge[domain].keys():\n","                for review_doc_id in self.knowledge_reader.get_review_doc_ids(domain, entity_id):\n","                    review_doc = self.knowledge_reader.get_review_doc(domain, entity_id, review_doc_id)\n","                    for review_sent_id, review_sent in review_doc['sentences'].items():\n","                        #i += 1\n","                        #knowledge.loc[i] = [used_knowledge]\n","                        result.append(\n","                            {'domain': domain, 'entity_id': entity_id, 'entity_name': review_doc['entity_name'],\n","                             'doc_id': f\"{review_doc_id}-{review_sent_id}\",\n","                             'doc': {'body': review_sent}})\n","                for faq_doc_id in self.knowledge_reader.get_faq_doc_ids(domain, entity_id):\n","                    #i += 1\n","                    faq_doc = self.knowledge_reader.get_faq_doc(domain, entity_id, faq_doc_id)\n","                    result.append({'domain': domain, 'entity_id': entity_id, 'entity_name': faq_doc['entity_name'],\n","                                   'doc_id': faq_doc_id,\n","                                   'doc': {'body': f\"{faq_doc['question']} {faq_doc['answer']}\"}})\n","\n","        return result\n","\n","    def _knowledge_to_string(self, doc, name=\"\"):\n","        \"\"\" Convert a knowledge snippet to a string \"\"\"\n","        doc_body = f\"{name.title()}: {doc['body']}\"\n","        return doc_body\n","\n","    def _create_examples(self):\n","        \"\"\" Creating examples for model training and evaluation \"\"\"\n","        self.examples = []\n","        idx = 0\n","        for dialog in tqdm(self.dialogs, disable=False, desc='creating examples'):\n","            dialog_id = dialog[\"id\"]\n","            label = dialog[\"label\"]\n","            dialog = dialog[\"log\"]\n","\n","            if label is None:\n","                # So we create dummy target here\n","                label = {\"target\": False}\n","\n","            target = label[\"target\"]\n","\n","            if not target and self.task != \"detection\":\n","                # we only care about non-knowledge-seeking turns in turn detection task\n","                continue\n","\n","            history = [turn[\"text\"] for turn in dialog[-3:]]\n","            \n","            gt_resp = label.get(\"response\", \"\")\n","\n","            if target:\n","                knowledge_keys = []\n","                knowledge_candidates = defaultdict(lambda: 0)\n","                used_knowledge = []\n","                candidates_text = []\n","                knowledge_prefix_visited = set()\n","\n","                if \"knowledge\" not in label:\n","                    raise ValueError(\"Please run entity matching before running knowledge selection\")\n","\n","                label_knowledge = label[\"knowledge\"]\n","\n","                for knowledge in label_knowledge:\n","                    if not (self.task == 'selection' and eval_only):\n","                        if knowledge['doc_type'] == 'review':\n","                            knowledge_key = f\"{knowledge['domain']}__{knowledge['entity_id']}__{knowledge['doc_id']}-{knowledge['sent_id']}\"\n","                        else:\n","                            knowledge_key = f\"{knowledge['domain']}__{knowledge['entity_id']}__{knowledge['doc_id']}\"\n","\n","                    # find snippets with same entity as candidates\n","                    prefix = \"{}__{}\".format(knowledge[\"domain\"], knowledge[\"entity_id\"])\n","                    if prefix not in knowledge_prefix_visited:\n","                        knowledge_prefix_visited.add(prefix)\n","                        _knowledge_candidates = [\n","                            cand\n","                            for cand in self.snippets.keys()\n","                            if \"__\".join(cand.split(\"__\")[:-1]) == prefix\n","                        ]\n","\n","                        for _knowledge_cand_idx, _knowledge_cand in enumerate(_knowledge_candidates):\n","                            knowledge_candidates[_knowledge_cand] = 1\n","\n","                    if self.split_type == \"train\" and self.negative_sample_method == \"oracle\":\n","                        # if there's not enough candidates during training, we just skip this example\n","                        if len(knowledge_candidates) < 2 or len(knowledge_candidates) <= len(label[\"knowledge\"]): #n_candidates : 2\n","                            continue\n","\n","                    if not (self.task == 'selection' and eval_only):\n","                        used_knowledge.append(self.snippets[knowledge_key])\n","                        knowledge_keys.append(knowledge_key)\n","                knowledge_candidates = [k for k, v in knowledge_candidates.items()]\n","                for k in knowledge_candidates : \n","                  candidates_text.append(self.snippets[k])\n","\n","            else:\n","                knowledge_candidates = None\n","                used_knowledge = []\n","                knowledge_keys = []\n","\n","            self.examples.append({\n","                \"history\": history,\n","                \"knowledge\": used_knowledge,\n","                \"knowledge_keys\": knowledge_keys,\n","                \"candidates\": knowledge_candidates,\n","                \"candidates_text\" : candidates_text,\n","                \"response_text\": gt_resp,\n","                \"label\": label,\n","                \"knowledge_seeking\": target,\n","                \"dialog_id\": dialog_id\n","            })\n","\n","            dstc11.loc[idx] = [history, knowledge_keys, used_knowledge, knowledge_candidates, candidates_text]\n","            if self.split_type == 'val' : \n","              dstc11_val.loc[idx] = [history, knowledge_keys, used_knowledge, knowledge_candidates, candidates_text]\n","            idx += 1\n","\n","    def __getitem__(self, index):\n","        raise NotImplementedError\n","\n","    def __len__(self):\n","        return len(self.examples)"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jhPExCNNlXdH","executionInfo":{"status":"ok","timestamp":1683521727660,"user_tz":-540,"elapsed":10,"user":{"displayName":"이하영","userId":"16656722516517244964"}}},"outputs":[],"source":["class DSTC11_Dataset(BaseDataset):\n","    def __init__(self, split_type, labels=True, labels_file=None):\n","        super(DSTC11_Dataset, self).__init__(split_type, labels, labels_file)\n","\n","    def __getitem__(self, index):\n","        example = self.examples[index]\n","        instance, _ = self.build_input_from_segments(\n","            example[\"knowledge\"],\n","            example[\"history\"],\n","        )\n","        print(instance)\n","        return instance\n","\n","class DSTC11_EvalDataset(BaseDataset):\n","    def __init__(self, split_type, labels=True, labels_file=None):\n","        super(DSTC11_EvalDataset, self).__init__(split_type, labels, labels_file)\n","\n","    def __getitem__(self, index):\n","        example = self.examples[index]\n","        return example\n","\n","    def collate_fn(self, batch):\n","        return batch"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96423,"status":"ok","timestamp":1683521824074,"user":{"displayName":"이하영","userId":"16656722516517244964"},"user_tz":-540},"id":"1ZNeaAdGlmgo","outputId":"e03ca316-8fc3-438b-de12-37edef400afd"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 28431/28431 [00:00<00:00, 1040506.23it/s]\n","creating examples: 100%|██████████| 28431/28431 [01:17<00:00, 364.56it/s]\n","100%|██████████| 4173/4173 [00:00<00:00, 974273.90it/s]\n","creating examples: 100%|██████████| 4173/4173 [00:10<00:00, 383.25it/s]\n"]}],"source":["train_dataset = DSTC11_Dataset(split_type = 'train')\n","valid_dataset = DSTC11_Dataset(split_type = 'val')"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":798},"executionInfo":{"elapsed":11820,"status":"ok","timestamp":1683521835868,"user":{"displayName":"이하영","userId":"16656722516517244964"},"user_tz":-540},"id":"82CT--RrTdRp","outputId":"bce6f2a2-c64b-487a-dd6a-fd63a9cb7be7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                 history  \\\n","0      [Do either of them have a 3 star rating?, Yes,...   \n","1      [I'm also looking for a restaurant by the name...   \n","2      [I want it moderately priced and I don't care ...   \n","3      [I'm looking for information on the cambridge ...   \n","4      [How about a gastropub restaurant?, I have two...   \n","...                                                  ...   \n","14763  [In the mid-range would be fine., Rajmahal is ...   \n","14764  [It should be in the west and have a star rati...   \n","14765  [I need the reservation for 2 people for 14:30...   \n","14766  [On Tuesday please., I've booked your room for...   \n","14767  [I am looking for an expensive Italian restaur...   \n","\n","                                          knowledge_keys  \\\n","0       [hotel__20__9-4, hotel__20__6-4, hotel__20__4-2]   \n","1       [restaurant__19250__0-3, restaurant__19250__0-4]   \n","2      [hotel__7__6-2, hotel__7__6-3, hotel__7__3-0, ...   \n","3                       [hotel__28__3-4, hotel__28__6-4]   \n","4      [restaurant__19188__0-3, restaurant__19188__1-...   \n","...                                                  ...   \n","14763  [restaurant__19274__3-1, restaurant__19274__3-...   \n","14764                   [hotel__17__0-2, hotel__17__6-1]   \n","14765  [restaurant__19182__0-2, restaurant__19182__2-...   \n","14766   [hotel__19__7-8, hotel__19__2-2, hotel__19__2-3]   \n","14767   [restaurant__19195__0-3, restaurant__19195__0-4]   \n","\n","                                               knowledge  \\\n","0      [Hobsons House: I also saw some hairs in the b...   \n","1      [Maharajah Tandoori Restaurant: First thing is...   \n","2      [Ashley Hotel: This place definitely delivered...   \n","3      [The Cambridge Belfry: One of the best things ...   \n","4      [Backstreet Bistro: It's a nice location and t...   \n","...                                                  ...   \n","14763  [Rajmahal: She was very nice and accommodating...   \n","14764  [Finches Bed And Breakfast: But where the room...   \n","14765  [The Golden Curry: The wait staff is courteous...   \n","14766  [Hamilton Lodge: The bed really needed a new m...   \n","14767  [Frankie And Bennys: Food quality was so-so an...   \n","\n","                                         candidates_keys  \\\n","0      [hotel__20__0-0, hotel__20__0-1, hotel__20__0-...   \n","1      [restaurant__19250__0-0, restaurant__19250__0-...   \n","2      [hotel__7__0-0, hotel__7__0-1, hotel__7__0-2, ...   \n","3      [hotel__28__0-0, hotel__28__0-1, hotel__28__1-...   \n","4      [restaurant__19188__0-0, restaurant__19188__0-...   \n","...                                                  ...   \n","14763  [restaurant__19274__0-0, restaurant__19274__0-...   \n","14764  [hotel__17__0-0, hotel__17__0-1, hotel__17__0-...   \n","14765  [restaurant__19182__0-0, restaurant__19182__0-...   \n","14766  [hotel__19__0-0, hotel__19__0-1, hotel__19__0-...   \n","14767  [restaurant__19195__0-0, restaurant__19195__0-...   \n","\n","                                              candidates  \\\n","0      [Hobsons House: I was very please with my rece...   \n","1      [Maharajah Tandoori Restaurant: My husband and...   \n","2      [Ashley Hotel: I enjoyed my breakfast choices ...   \n","3      [The Cambridge Belfry: My girlfriend and I enj...   \n","4      [Backstreet Bistro: My friends and I went into...   \n","...                                                  ...   \n","14763  [Rajmahal: I visited Rajmahal recently by myse...   \n","14764  [Finches Bed And Breakfast: I'm conflicted abo...   \n","14765  [The Golden Curry: If you're looking for a pla...   \n","14766  [Hamilton Lodge: I was here on business., Hami...   \n","14767  [Frankie And Bennys: Frankie and Benny's is ri...   \n","\n","                                                neg_keys  \\\n","0      [hotel__20__0-0, hotel__20__0-1, hotel__20__0-...   \n","1      [restaurant__19250__0-0, restaurant__19250__0-...   \n","2      [hotel__7__0-0, hotel__7__0-1, hotel__7__0-2, ...   \n","3      [hotel__28__0-0, hotel__28__0-1, hotel__28__1-...   \n","4      [restaurant__19188__0-0, restaurant__19188__0-...   \n","...                                                  ...   \n","14763  [restaurant__19274__0-0, restaurant__19274__0-...   \n","14764  [hotel__17__0-0, hotel__17__0-1, hotel__17__0-...   \n","14765  [restaurant__19182__0-0, restaurant__19182__0-...   \n","14766  [hotel__19__0-0, hotel__19__0-1, hotel__19__0-...   \n","14767  [restaurant__19195__0-0, restaurant__19195__0-...   \n","\n","                                             neg_samples  \n","0      [Hobsons House: I was very please with my rece...  \n","1      [Maharajah Tandoori Restaurant: My husband and...  \n","2      [Ashley Hotel: I enjoyed my breakfast choices ...  \n","3      [The Cambridge Belfry: My girlfriend and I enj...  \n","4      [Backstreet Bistro: My friends and I went into...  \n","...                                                  ...  \n","14763  [Rajmahal: I visited Rajmahal recently by myse...  \n","14764  [Finches Bed And Breakfast: I'm conflicted abo...  \n","14765  [The Golden Curry: If you're looking for a pla...  \n","14766  [Hamilton Lodge: I was here on business., Hami...  \n","14767  [Frankie And Bennys: Frankie and Benny's is ri...  \n","\n","[14768 rows x 7 columns]"],"text/html":["\n","  <div id=\"df-a2828943-d3e3-497d-b94e-c5f5f438231f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>history</th>\n","      <th>knowledge_keys</th>\n","      <th>knowledge</th>\n","      <th>candidates_keys</th>\n","      <th>candidates</th>\n","      <th>neg_keys</th>\n","      <th>neg_samples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Do either of them have a 3 star rating?, Yes,...</td>\n","      <td>[hotel__20__9-4, hotel__20__6-4, hotel__20__4-2]</td>\n","      <td>[Hobsons House: I also saw some hairs in the b...</td>\n","      <td>[hotel__20__0-0, hotel__20__0-1, hotel__20__0-...</td>\n","      <td>[Hobsons House: I was very please with my rece...</td>\n","      <td>[hotel__20__0-0, hotel__20__0-1, hotel__20__0-...</td>\n","      <td>[Hobsons House: I was very please with my rece...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[I'm also looking for a restaurant by the name...</td>\n","      <td>[restaurant__19250__0-3, restaurant__19250__0-4]</td>\n","      <td>[Maharajah Tandoori Restaurant: First thing is...</td>\n","      <td>[restaurant__19250__0-0, restaurant__19250__0-...</td>\n","      <td>[Maharajah Tandoori Restaurant: My husband and...</td>\n","      <td>[restaurant__19250__0-0, restaurant__19250__0-...</td>\n","      <td>[Maharajah Tandoori Restaurant: My husband and...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[I want it moderately priced and I don't care ...</td>\n","      <td>[hotel__7__6-2, hotel__7__6-3, hotel__7__3-0, ...</td>\n","      <td>[Ashley Hotel: This place definitely delivered...</td>\n","      <td>[hotel__7__0-0, hotel__7__0-1, hotel__7__0-2, ...</td>\n","      <td>[Ashley Hotel: I enjoyed my breakfast choices ...</td>\n","      <td>[hotel__7__0-0, hotel__7__0-1, hotel__7__0-2, ...</td>\n","      <td>[Ashley Hotel: I enjoyed my breakfast choices ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[I'm looking for information on the cambridge ...</td>\n","      <td>[hotel__28__3-4, hotel__28__6-4]</td>\n","      <td>[The Cambridge Belfry: One of the best things ...</td>\n","      <td>[hotel__28__0-0, hotel__28__0-1, hotel__28__1-...</td>\n","      <td>[The Cambridge Belfry: My girlfriend and I enj...</td>\n","      <td>[hotel__28__0-0, hotel__28__0-1, hotel__28__1-...</td>\n","      <td>[The Cambridge Belfry: My girlfriend and I enj...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[How about a gastropub restaurant?, I have two...</td>\n","      <td>[restaurant__19188__0-3, restaurant__19188__1-...</td>\n","      <td>[Backstreet Bistro: It's a nice location and t...</td>\n","      <td>[restaurant__19188__0-0, restaurant__19188__0-...</td>\n","      <td>[Backstreet Bistro: My friends and I went into...</td>\n","      <td>[restaurant__19188__0-0, restaurant__19188__0-...</td>\n","      <td>[Backstreet Bistro: My friends and I went into...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>14763</th>\n","      <td>[In the mid-range would be fine., Rajmahal is ...</td>\n","      <td>[restaurant__19274__3-1, restaurant__19274__3-...</td>\n","      <td>[Rajmahal: She was very nice and accommodating...</td>\n","      <td>[restaurant__19274__0-0, restaurant__19274__0-...</td>\n","      <td>[Rajmahal: I visited Rajmahal recently by myse...</td>\n","      <td>[restaurant__19274__0-0, restaurant__19274__0-...</td>\n","      <td>[Rajmahal: I visited Rajmahal recently by myse...</td>\n","    </tr>\n","    <tr>\n","      <th>14764</th>\n","      <td>[It should be in the west and have a star rati...</td>\n","      <td>[hotel__17__0-2, hotel__17__6-1]</td>\n","      <td>[Finches Bed And Breakfast: But where the room...</td>\n","      <td>[hotel__17__0-0, hotel__17__0-1, hotel__17__0-...</td>\n","      <td>[Finches Bed And Breakfast: I'm conflicted abo...</td>\n","      <td>[hotel__17__0-0, hotel__17__0-1, hotel__17__0-...</td>\n","      <td>[Finches Bed And Breakfast: I'm conflicted abo...</td>\n","    </tr>\n","    <tr>\n","      <th>14765</th>\n","      <td>[I need the reservation for 2 people for 14:30...</td>\n","      <td>[restaurant__19182__0-2, restaurant__19182__2-...</td>\n","      <td>[The Golden Curry: The wait staff is courteous...</td>\n","      <td>[restaurant__19182__0-0, restaurant__19182__0-...</td>\n","      <td>[The Golden Curry: If you're looking for a pla...</td>\n","      <td>[restaurant__19182__0-0, restaurant__19182__0-...</td>\n","      <td>[The Golden Curry: If you're looking for a pla...</td>\n","    </tr>\n","    <tr>\n","      <th>14766</th>\n","      <td>[On Tuesday please., I've booked your room for...</td>\n","      <td>[hotel__19__7-8, hotel__19__2-2, hotel__19__2-3]</td>\n","      <td>[Hamilton Lodge: The bed really needed a new m...</td>\n","      <td>[hotel__19__0-0, hotel__19__0-1, hotel__19__0-...</td>\n","      <td>[Hamilton Lodge: I was here on business., Hami...</td>\n","      <td>[hotel__19__0-0, hotel__19__0-1, hotel__19__0-...</td>\n","      <td>[Hamilton Lodge: I was here on business., Hami...</td>\n","    </tr>\n","    <tr>\n","      <th>14767</th>\n","      <td>[I am looking for an expensive Italian restaur...</td>\n","      <td>[restaurant__19195__0-3, restaurant__19195__0-4]</td>\n","      <td>[Frankie And Bennys: Food quality was so-so an...</td>\n","      <td>[restaurant__19195__0-0, restaurant__19195__0-...</td>\n","      <td>[Frankie And Bennys: Frankie and Benny's is ri...</td>\n","      <td>[restaurant__19195__0-0, restaurant__19195__0-...</td>\n","      <td>[Frankie And Bennys: Frankie and Benny's is ri...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>14768 rows × 7 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a2828943-d3e3-497d-b94e-c5f5f438231f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a2828943-d3e3-497d-b94e-c5f5f438231f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a2828943-d3e3-497d-b94e-c5f5f438231f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}],"source":["neg_idx = [[] for _ in range(len(dstc11))]\n","neg_text = [[] for _ in range(len(dstc11))]\n","for idx, cand_keys in enumerate(dstc11['candidates_keys']) :\n","  for i, key in enumerate(cand_keys):\n","    if key not in dstc11['knowledge_keys'][idx] :\n","      neg_idx[idx].append(key)\n","      neg_text[idx].append(dstc11['candidates'][idx][i])\n","dstc11['neg_keys'] = neg_idx\n","dstc11['neg_samples'] = neg_text\n","dstc11"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6548,"status":"ok","timestamp":1683521842409,"user":{"displayName":"이하영","userId":"16656722516517244964"},"user_tz":-540},"id":"rVXDsz-R3GXu","outputId":"841884a8-7d20-49e5-8ce8-1fc2aa51520e"},"outputs":[{"output_type":"stream","name":"stdout","text":["                                             history  \\\n","0  Do either of them have a 3 star rating?[SEP]Ye...   \n","1  I'm also looking for a restaurant by the name ...   \n","2  I want it moderately priced and I don't care w...   \n","3  I'm looking for information on the cambridge b...   \n","4  How about a gastropub restaurant?[SEP]I have t...   \n","\n","                                      knowledge_keys  \\\n","0   [hotel__20__9-4, hotel__20__6-4, hotel__20__4-2]   \n","1   [restaurant__19250__0-3, restaurant__19250__0-4]   \n","2  [hotel__7__6-2, hotel__7__6-3, hotel__7__3-0, ...   \n","3                   [hotel__28__3-4, hotel__28__6-4]   \n","4  [restaurant__19188__0-3, restaurant__19188__1-...   \n","\n","                                           knowledge  \\\n","0  [Hobsons House: I also saw some hairs in the b...   \n","1  [Maharajah Tandoori Restaurant: First thing is...   \n","2  [Ashley Hotel: This place definitely delivered...   \n","3  [The Cambridge Belfry: One of the best things ...   \n","4  [Backstreet Bistro: It's a nice location and t...   \n","\n","                                     candidates_keys  \\\n","0  [hotel__20__0-0, hotel__20__0-1, hotel__20__0-...   \n","1  [restaurant__19250__0-0, restaurant__19250__0-...   \n","2  [hotel__7__0-0, hotel__7__0-1, hotel__7__0-2, ...   \n","3  [hotel__28__0-0, hotel__28__0-1, hotel__28__1-...   \n","4  [restaurant__19188__0-0, restaurant__19188__0-...   \n","\n","                                          candidates  \\\n","0  [Hobsons House: I was very please with my rece...   \n","1  [Maharajah Tandoori Restaurant: My husband and...   \n","2  [Ashley Hotel: I enjoyed my breakfast choices ...   \n","3  [The Cambridge Belfry: My girlfriend and I enj...   \n","4  [Backstreet Bistro: My friends and I went into...   \n","\n","                                            neg_keys  \\\n","0  [hotel__20__0-0, hotel__20__0-1, hotel__20__0-...   \n","1  [restaurant__19250__0-0, restaurant__19250__0-...   \n","2  [hotel__7__0-0, hotel__7__0-1, hotel__7__0-2, ...   \n","3  [hotel__28__0-0, hotel__28__0-1, hotel__28__1-...   \n","4  [restaurant__19188__0-0, restaurant__19188__0-...   \n","\n","                                         neg_samples  \n","0  [Hobsons House: I was very please with my rece...  \n","1  [Maharajah Tandoori Restaurant: My husband and...  \n","2  [Ashley Hotel: I enjoyed my breakfast choices ...  \n","3  [The Cambridge Belfry: My girlfriend and I enj...  \n","4  [Backstreet Bistro: My friends and I went into...  \n"]}],"source":["idx = 0\n","for i in dstc11['history']:\n","  dstc11['history'].loc[idx] = '[SEP]'.join(s for s in i)\n","  idx += 1\n","\n","print(dstc11.head())"]},{"cell_type":"code","source":["idx = 0\n","for i in dstc11_val['history']:\n","  dstc11_val['history'].loc[idx] = '[SEP]'.join(s for s in i)\n","  idx += 1"],"metadata":{"id":"kmvrl28Gg-lL","executionInfo":{"status":"ok","timestamp":1683521842414,"user_tz":-540,"elapsed":37,"user":{"displayName":"이하영","userId":"16656722516517244964"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","execution_count":13,"metadata":{"id":"UTX0L8u73XqT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683521842414,"user_tz":-540,"elapsed":36,"user":{"displayName":"이하영","userId":"16656722516517244964"}},"outputId":"b1610e14-dc50-462b-b7b9-8a7befdf69a4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2129"]},"metadata":{},"execution_count":13}],"source":["history = list(dstc11['history'])\n","history_val = list(dstc11_val['history'])\n","len(history_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7lzfIbJg4qT"},"outputs":[],"source":["cand_keys = []\n","querys = []\n","target = [] # knowledge면 1, 아니면 0\n","for idx, know_key in enumerate(dstc11['knowledge_keys']) :\n","  for i, k in enumerate(know_key) :\n","    cand_keys.append(dstc11['knowledge'][idx][i])\n","    querys.append(idx)\n","    target.append(1)\n","  \n","  for _ in range(len(know_key)) :\n","    i = random.randint(0, len(dstc11['neg_keys'][idx])-1)\n","    cand_keys.append(dstc11['neg_samples'][idx][i])\n","    querys.append(idx)\n","    target.append(0)\n","dpr_dataset = pd.DataFrame({'candidates' : cand_keys, 'target' : target, 'query_id' : querys })"]},{"cell_type":"code","source":["dstc11_val"],"metadata":{"id":"x2rqo--F6jOV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tmp = 0\n","for i in range(len(knowledge_val)) :\n","  tmp += len(knowledge_val[i])\n","print(tmp)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"we22gHNjuPJQ","executionInfo":{"status":"ok","timestamp":1683521884711,"user_tz":-540,"elapsed":4,"user":{"displayName":"이하영","userId":"16656722516517244964"}},"outputId":"28d4d7f9-8665-487e-f398-e5a07945e2a3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["202137\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"TXCLcyhHjjj-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683521847011,"user_tz":-540,"elapsed":3,"user":{"displayName":"이하영","userId":"16656722516517244964"}},"outputId":"4208518e-7844-40a2-c606-2d8ab8ff8ee0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2129"]},"metadata":{},"execution_count":15}],"source":["knowledge = dpr_dataset['candidates'].tolist()\n","knowledge_val = dstc11_val['candidates'].tolist()\n","len(knowledge_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLZ67zgywlyN"},"outputs":[],"source":["from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n","from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n","from torch.utils.data import TensorDataset\n","\n","Qmodel_name = \"facebook/dpr-question_encoder-single-nq-base\"\n","Cmodel_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n","\n","q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(Qmodel_name)\n","c_tokenizer = DPRContextEncoderTokenizer.from_pretrained(Cmodel_name)\n","\n","q_encoder = DPRQuestionEncoder.from_pretrained(Qmodel_name)\n","c_encoder = DPRContextEncoder.from_pretrained(Cmodel_name)\n","\n","q_encoder.train()\n","c_encoder.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBxfkbTWjcmJ"},"outputs":[],"source":["train_query_encodings = q_tokenizer(history, padding = True, truncation = True, return_tensors = 'pt')\n","train_context_encodings = c_tokenizer(knowledge, padding = True, truncation = True, return_tensors = 'pt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3VUUHITXewm"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8z1-CSA-xcYy"},"outputs":[],"source":["class DPR(nn.Module):\n","\n","\n","  def __init__(self, query_model, passage_model, query_tokenizer, passage_tokenizer, \n","              dense_size, freeze_params = 0.0, batch_size = 2):\n","    \n","    '''\n","    :query_model : The model that encodes queries to dense representation\n","    :passage_model : The model that encodes passages to dense representation\n","    :query_tokenizer : tokenizer for queries\n","    :passage_tokenizer : tokenizer for passages\n","    :passage_dict : dictionary of passages with their unique id\n","    :questions : A list of tuples with question and their correct passage id\n","    :dense_size : the dimension to which the DPR has to encode\n","    :freeze_params : the percentage of the parameters to be frozen\n","    :batch_size : the batch size for training\n","    :sample_size: the sample size for negative sampling\n","    '''\n","    super(DPR, self).__init__()\n","    self.query_model = query_model\n","    self.query_tokenizer = query_tokenizer\n","    self.passage_model = passage_model\n","    self.passage_tokenizer = passage_tokenizer\n","    self.freeze_params = freeze_params\n","    #self.sample_size = sample_size\n","    self.batch_size = batch_size\n","\n","    self.passage_to_dense = nn.Sequential(nn.Linear(768, dense_size * 2),\n","                                          nn.ReLU(),\n","                                          nn.Linear(dense_size * 2, dense_size),\n","                                          nn.GELU())\n","    \n","    self.query_to_dense = nn.Sequential(nn.Linear(768, dense_size * 2),\n","                                          nn.ReLU(),\n","                                          nn.Linear(dense_size * 2, dense_size),\n","                                          nn.GELU())\n","    self.log_softmax = nn.LogSoftmax(dim = 1)\n","    self.freeze_layers()\n","\n","\n","  # Freeze the first self.freeze_params % layers\n","  def freeze_layers(self):\n","    num_query_layers = sum(1 for _ in self.query_model.parameters())\n","    num_passage_layers = sum(1 for _ in self.passage_model.parameters())\n","\n","    for parameters in list(self.query_model.parameters())[:int(self.freeze_params * num_query_layers)]:\n","      parameters.requires_grad = False\n","\n","    for parameters in list(self.query_model.parameters())[int(self.freeze_params * num_query_layers):]:\n","      parameters.requires_grad = True\n","\n","    for parameters in list(self.passage_model.parameters())[:int(self.freeze_params * num_passage_layers)]:\n","      parameters.requires_grad = False\n","\n","    for parameters in list(self.passage_model.parameters())[int(self.freeze_params * num_passage_layers):]:\n","      parameters.requires_grad = True\n","\n","  def get_passage_vectors(self, passage):\n","    p_vector = self.passage_model(input_ids = passage.input_ids, \n","                                  attention_mask = passage.attention_mask)\n","    p_vector = self.query_to_dense(p_vector.pooler_output)\n","    return p_vector\n","\n","  def get_query_vector(self, query):\n","    q_vector = self.query_model(input_ids = query.input_ids, \n","                                attention_mask = query.attention_mask)\n","    q_vector = self.query_to_dense(q_vector.pooler_output)\n","    return q_vector\n","\n","  def dot_product(self, q_vector, p_vector):\n","    q_vector = q_vector.unsqueeze(1)\n","    sim = torch.matmul(q_vector, torch.transpose(p_vector, -2, -1))\n","    return sim\n","\n","  def forward(self, context_input_ids, context_attention_mask, query_input_ids, query_attention_mask):\n","    dense_passage = self.passage_model(input_ids = context_input_ids, attention_mask = context_attention_mask)\n","    dense_query = self.query_model(input_ids = query_input_ids, attention_mask = query_attention_mask)\n","    dense_passage = dense_passage['pooler_output']\n","    dense_query = dense_query['pooler_output']\n","    dense_passage = self.passage_to_dense(dense_passage)\n","    dense_query = self.query_to_dense(dense_query)\n","    similarity_score = self.dot_product(dense_query, dense_passage)\n","    similarity_score = similarity_score.squeeze(1)\n","    logits = self.log_softmax(similarity_score)\n","    return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1683424575688,"user":{"displayName":"이하영","userId":"16656722516517244964"},"user_tz":-540},"id":"r6ggzRVbXkVO","outputId":"a4138121-b3d3-4395-bdfa-79a8bd07081c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(217996672, 124251520)"]},"metadata":{},"execution_count":23}],"source":["dpr_model = DPR(query_model = q_encoder, \n","                passage_model = c_encoder, \n","                query_tokenizer = q_tokenizer, \n","                passage_tokenizer = c_tokenizer, \n","                dense_size = 64,\n","                freeze_params = 0.3,\n","                batch_size = 4)\n","\n","dpr_model.train()\n","\n","sum(p.numel() for p in dpr_model.parameters()), sum(p.numel() for p in dpr_model.parameters() if p.requires_grad == True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1683424575689,"user":{"displayName":"이하영","userId":"16656722516517244964"},"user_tz":-540},"id":"UWQyH5ahXtlq","outputId":"9d5477c5-2df6-442c-db71-ae857451312f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":24}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaMjYShxXxDi"},"outputs":[],"source":["criterion = nn.NLLLoss()\n","optimizer = torch.optim.AdamW(dpr_model.parameters(), lr = 5e-5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1683424575689,"user":{"displayName":"이하영","userId":"16656722516517244964"},"user_tz":-540},"id":"vpTBIGK9Xza4","outputId":"9816f700-a2d1-44c8-d57b-32c407cbe053"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["113488"]},"metadata":{},"execution_count":26}],"source":["batch_size = 32\n","num_questions = len(train_context_encodings.input_ids)\n","num_questions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rR4_plFQYO-l"},"outputs":[],"source":["def get_batch_train(num):\n","  true = []\n","  context_input_ids_tensor = []\n","  context_attention_mask_tensor = []\n","  optimizer.zero_grad()\n","\n","  for i in range(batch_size) : \n","    idx = num + i\n","    query_idx = dpr_dataset['query_id'][idx]\n","    context_input_ids_tensor.append(train_context_encodings.input_ids[idx])\n","    context_attention_mask_tensor.append(train_context_encodings.attention_mask[idx])\n","    query_input_ids = train_query_encodings.input_ids[query_idx]\n","    query_attention_mask = train_query_encodings.attention_mask[query_idx]\n","\n","    target = dpr_dataset['target'][idx]\n","    if target == 1 :\n","      true.append(1)\n","    else :\n","      true.append(0)\n","  \n","  context_input_ids_tensor = torch.stack(context_input_ids_tensor)\n","  context_attention_mask_tensor = torch.stack(context_attention_mask_tensor)\n","  query_input_ids = query_input_ids.unsqueeze(0)\n","  query_attention_mask = query_attention_mask.unsqueeze(0)\n","\n","  return context_input_ids_tensor, context_attention_mask_tensor, query_input_ids, query_attention_mask, true"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tb3D79XaoVHd","executionInfo":{"status":"ok","timestamp":1683508397946,"user_tz":-540,"elapsed":81706394,"user":{"displayName":"이하영","userId":"16656722516517244964"}},"outputId":"a1348381-1978-4f9b-a8f5-22601c0e30b6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch : 0\n","len : 3546\n","Batch : 0  Loss : 0.0033298861980438233\n","Batch : 1  Loss : 3.4686846764087678\n","Batch : 2  Loss : 3.470310158729553\n","Batch : 3  Loss : 3.467512248516083\n","Evaluation\n","top-5 acc:  0.18788163457022078\n","top-20 acc:  0.7515265382808831\n","top-100 acc:  4.3212775951150775\n","Epoch : 1\n","len : 3546\n","Batch : 0  Loss : 1.892682545185089\n","Batch : 1  Loss : 3.466170468568802\n","Batch : 2  Loss : 3.46631063580513\n","Batch : 3  Loss : 3.4662934935092924\n","Evaluation\n","top-5 acc:  0.3287928604978863\n","top-20 acc:  1.0803193987787694\n","top-100 acc:  4.274307186472522\n","Epoch : 2\n","len : 3546\n","Batch : 0  Loss : 1.8943127226829528\n","Batch : 1  Loss : 3.4669907686710357\n","Batch : 2  Loss : 3.465662769794464\n","Batch : 3  Loss : 3.4661018614768984\n","Evaluation\n","top-5 acc:  0.18788163457022078\n","top-20 acc:  0.9394081728511038\n","top-100 acc:  4.274307186472522\n","Epoch : 3\n","len : 3546\n","Batch : 0  Loss : 1.892113411426544\n","Batch : 1  Loss : 3.466515021085739\n","Batch : 2  Loss : 3.4658370172977446\n","Batch : 3  Loss : 3.4662423129081725\n","Evaluation\n","top-5 acc:  0.14091122592766556\n","top-20 acc:  0.8454673555659934\n","top-100 acc:  4.227336777829968\n","Epoch : 4\n","len : 3546\n","Batch : 0  Loss : 1.8927081999778748\n","Batch : 1  Loss : 3.4658911015987397\n","Batch : 2  Loss : 3.4665511360168457\n","Batch : 3  Loss : 3.466652023553848\n","Evaluation\n","top-5 acc:  0.14091122592766556\n","top-20 acc:  0.5636449037106622\n","top-100 acc:  4.133395960544857\n"]}],"source":["batch_loss = 0\n","num_train_epochs = 5\n","\n","q_encoder.zero_grad()\n","c_encoder.zero_grad()\n","torch.cuda.empty_cache()\n","\n","#train_iterator = trange(int(num_train_epochs), desc = \"Epoch\")\n","for idx in range(num_train_epochs):\n","  print(f\"Epoch : {idx}\")\n","  print(f\"len : {num_questions // batch_size}\")\n","  for i, batch in enumerate(range(num_questions // batch_size)) :\n","    q_encoder.train()\n","    c_encoder.train()\n","    if torch.cuda.is_available() :\n","      context_input_ids_tensor, context_attention_mask_tensor, query_input_ids, query_attention_mask, true = get_batch_train(batch * batch_size)\n","      pred = dpr_model(context_input_ids_tensor, context_attention_mask_tensor, query_input_ids, query_attention_mask)\n","      \n","      true = torch.tensor([0])\n","      loss = criterion(pred, true)\n","      loss.backward()\n","      batch_loss += loss.item()\n","      optimizer.step()\n","\n","      q_encoder.zero_grad()\n","      c_encoder.zero_grad()\n","      torch.cuda.empty_cache()\n","\n","      if i%1000 == 0:\n","        print(f\"Batch : {int(i/1000)}  Loss : {batch_loss/1000}\")\n","        batch_loss = 0\n","\n","  print(\"Evaluation\")     \n","  with torch.no_grad():\n","    c_encoder.eval()\n","    q_encoder.eval()\n","\n","    # top ~ acc 구하기\n","    top_5 = 0\n","    top_20 = 0\n","    top_100 = 0\n","\n","    val_num = len(history_val)\n","    for i in range(len(history_val)) :\n","        #print(history_val[i], dstc11['knowledge'][i])\n","        val_query_encodings = q_tokenizer(history_val[i], padding = True, truncation = True, return_tensors = 'pt')\n","        q_emb = q_encoder(input_ids = val_query_encodings.input_ids, \n","                          attention_mask = val_query_encodings.attention_mask).pooler_output.to(device)\n","\n","        val_context_encodings = c_tokenizer(dstc11['candidates'][i], padding = True, truncation = True, return_tensors = 'pt')\n","        c_emb = c_encoder(input_ids = val_context_encodings.input_ids,\n","                          attention_mask = val_context_encodings.attention_mask).pooler_output.to(device)\n","        #c_emb = torch.Tensor(c_emb).squeeze()\n","\n","        sim = torch.matmul(q_emb, torch.transpose(c_emb, 0, 1))\n","        rank = torch.argsort(sim, dim=1, descending=True).squeeze()\n","\n","        if i in rank[0:5]: \n","            top_5 += 1\n","        if i in rank[0:20]: \n","            top_20 += 1\n","        if i in rank[0:100]: \n","            top_100 += 1\n","\n","    print('top-5 acc: ', top_5/val_num * 100)\n","    print('top-20 acc: ', top_20/val_num * 100)\n","    print('top-100 acc: ', top_100/val_num * 100)"]},{"cell_type":"markdown","metadata":{"id":"h58651dTlYCU"},"source":["___________________________________________"]},{"cell_type":"code","source":["with torch.no_grad():\n","    c_encoder.eval()\n","    q_encoder.eval()\n","\n","    # top ~ acc 구하기\n","    top_5 = 0\n","    top_20 = 0\n","    top_100 = 0\n","\n","    val_num = len(history_val)\n","    for i in range(len(history_val)) :\n","        #print(history_val[i], dstc11['knowledge'][i])\n","        val_query_encodings = q_tokenizer(history_val[i], padding = True, truncation = True, return_tensors = 'pt')\n","        q_emb = q_encoder(input_ids = val_query_encodings.input_ids, \n","                          attention_mask = val_query_encodings.attention_mask).pooler_output.to(device)\n","\n","        val_context_encodings = c_tokenizer(dstc11['candidates'][i], padding = True, truncation = True, return_tensors = 'pt')\n","        c_emb = c_encoder(input_ids = val_context_encodings.input_ids,\n","                          attention_mask = val_context_encodings.attention_mask).pooler_output.to(device)\n","        c_emb = torch.Tensor(c_emb).squeeze()\n","\n","        sim = torch.matmul(q_emb, torch.transpose(c_emb, 0, 1))\n","        rank = torch.argsort(sim, dim=1, descending=True).squeeze()\n","        print(rank)\n","\n","        if i in rank[0:5]: \n","            top_5 += 1\n","        if i in rank[0:20]: \n","            top_20 += 1\n","        if i in rank[0:100]: \n","            top_100 += 1\n","\n","    print('top-5 acc: ', top_5/val_num * 100)\n","    print('top-20 acc: ', top_20/val_num * 100)\n","    print('top-100 acc: ', top_100/val_num * 100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3qCfyvwmCsR1","executionInfo":{"status":"error","timestamp":1683426598435,"user_tz":-540,"elapsed":13978,"user":{"displayName":"이하영","userId":"16656722516517244964"}},"outputId":"6021686a-c6b0-4410-e95e-07189b9297d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([84, 78, 56, 70, 16, 89, 65, 69, 90, 45, 39, 88, 25, 67, 55, 59, 58, 12,\n","        66, 71, 28, 53, 75, 80, 29, 64, 63, 61, 60, 23, 11, 68, 72, 62,  1,  7,\n","        10, 81, 57, 24, 74, 77, 83, 82, 54, 44, 38, 37, 76,  2,  9, 33, 85, 47,\n","        18, 34,  8, 46, 43, 32, 51, 31,  0, 19, 86, 73, 49, 13,  3, 17, 15, 22,\n","        20, 41, 21, 87,  4, 42, 79, 27, 14,  5, 36, 50, 40, 52, 26,  6, 30, 48,\n","        35], device='cuda:0')\n","tensor([28, 22, 15, 30,  5,  9, 12, 18, 11, 61, 48, 26, 20,  4, 55, 39, 27, 40,\n","        60, 44,  7, 58, 57, 34,  0, 25, 37,  1, 53, 50,  8, 13, 16,  6, 67, 17,\n","        41, 56, 49, 43, 29, 10,  3, 36, 31, 63, 23, 72, 14, 64, 46, 75,  2, 33,\n","        62, 47, 24, 42, 45, 54, 52, 51, 32, 19, 59, 68, 21, 77, 66, 35, 65, 73,\n","        76, 38, 69, 71, 74, 70], device='cuda:0')\n","tensor([86, 58, 84, 54, 87, 43, 29, 85, 66, 62,  7, 56, 49, 90, 61, 33, 68,  9,\n","        35, 44, 60,  3, 72, 36,  1, 45, 10, 26, 28, 83, 48, 34, 55, 42, 19, 52,\n","        70, 21, 14, 63, 67, 31, 16, 25, 78,  8, 15, 91, 65, 74, 22, 17, 57, 50,\n","        20,  5, 69, 38,  2, 27, 11, 12, 40, 81, 24, 51, 13, 79, 47, 64, 71, 39,\n","        41, 89,  6, 37, 53,  4, 73, 59, 82, 92, 32, 88, 76,  0, 46, 30, 18, 77,\n","        23, 80, 75], device='cuda:0')\n","tensor([34, 18, 85, 50, 58, 77, 62, 73, 64, 27, 56, 24, 70, 59,  0, 45, 68, 54,\n","         9, 41, 46, 74, 61, 88, 38, 31, 17, 78, 23, 82, 39, 22, 75, 60, 81, 20,\n","        25, 71, 84, 80, 66, 69, 83, 65, 33, 37, 14, 63, 49, 72, 36, 67, 87,  2,\n","        42, 29, 57, 86, 76, 79, 55, 52, 21, 40, 44, 91,  7, 35, 11, 28, 47, 48,\n","        26, 43, 16, 90, 19, 30,  6, 89,  1,  5, 13, 12,  4, 32,  8,  3, 15, 10,\n","        51, 53], device='cuda:0')\n","tensor([ 35,  39,  25, 124,  12,   1, 107,  70,  96,  24,  34,  84,  19, 118,\n","        103,  48,   4, 142,  45,   0,  13,  42, 101,  66,  41,  43,  71,  65,\n","         85,   6, 110, 149,  47,  36,  33,  88,  83, 121, 140,  44,   7,  99,\n","          3,  37,  18, 150, 112,  11,  91, 113,  28,  51,  72,  86, 127, 133,\n","         90, 128,  52,  87,  80,  59,  73,  30, 117,  75,  15,  22,  23,  38,\n","        141,  29, 109,  46,  17, 102,  21, 134,   9, 132,   2,  76, 122,  78,\n","         94, 105,  40,  97,  10, 147,  55,  16, 138, 130,   5,  50,  54,  49,\n","        126, 108, 100,  60,  77,  20, 129, 131,  63, 115,  68,  58,  27,  89,\n","          8,  67, 145, 111,  69, 143, 106,  82,  14,  32,  74,  79, 137, 144,\n","         53, 119,  98, 146,  93, 135,  64, 120, 116,  81, 148,  56, 136,  61,\n","         31,  26,  62, 151, 114, 104, 139, 125,  92,  95,  57, 123],\n","       device='cuda:0')\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-27a08c572308>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mval_context_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdstc11\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'candidates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         c_emb = c_encoder(input_ids = val_context_encodings.input_ids,\n\u001b[0m\u001b[1;32m     19\u001b[0m                           attention_mask = val_context_encodings.attention_mask).pooler_output.to(device)\n\u001b[1;32m     20\u001b[0m         \u001b[0mc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/dpr/modeling_dpr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         outputs = self.ctx_encoder(\n\u001b[0m\u001b[1;32m    507\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/dpr/modeling_dpr.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     ) -> Union[BaseModelOutputWithPooling, Tuple[Tensor, ...]]:\n\u001b[0;32m--> 196\u001b[0;31m         outputs = self.bert_model(\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m         )\n\u001b[0;32m-> 1020\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    608\u001b[0m                 )\n\u001b[1;32m    609\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    611\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 425\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    426\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"fLLRVC7lCuJJ"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyPUHS4txQvINd66tzA1k+7i"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}